{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4f74d3-5fef-4786-ab6b-c7296f240615",
   "metadata": {},
   "source": [
    "# **Fine-tuning RoBERTa for named-entity recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e028e0-376b-4f1f-9aa3-54cb80b5115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad8c0d4-3187-47ef-9c1d-9c251c1a2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859f362e-cabc-47e0-bd92-84ce117b6870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shjiang/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-03 22:30:45.685768: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-03 22:30:46.933101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from seqeval.metrics import classification_report\n",
    "import math\n",
    "import os\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654de60e-abe4-4bec-8010-5e1cb4549d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep  3 21:37:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 14%   27C    P0    56W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 13%   27C    P5    12W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 12%   25C    P5    12W / 250W |      0MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:D9:00.0 Off |                  N/A |\n",
      "| 11%   28C    P0    52W / 250W |      0MiB / 11264MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c235a2e-b218-433d-8394-49e64398cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 10# 5 for more than 1024, 10 for less\n",
    "LEARNING_RATE = 5e-5 #1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "MAX_LEN = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79babb2-90d0-4e0d-a65b-0e4e6af351a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocessing the dataset\n",
    "\n",
    "Named entity recognition (NER) uses a specific annotation scheme, which is defined (at least for European languages) at the word level. An annotation scheme that is widely used is called IOB-tagging, which stands for Inside-Outside-Beginning. Each tag indicates whether the corresponding word is inside, outside or at the beginning of a specific named entity. The reason this is used is because named entities usually comprise more than 1 word.\n",
    "\n",
    "Let's have a look at an example. If you have a sentence like \"Barack Obama was born in Hawaï\", then the corresponding tags would be [B-PERS, I-PERS, O, O, O, B-GEO]. B-PERS means that the word \"Barack\" is the beginning of a person, I-PERS means that the word \"Obama\" is inside a person, \"O\" means that the word \"was\" is outside a named entity, and so on. So one typically has as many tags as there are words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd49a5c3-b9fc-4d40-867a-9b9175ae13f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13653"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"cleaned_plain-text_labeled_term+combined_no_ref_no_cit_def_same_len_only.csv\", delimiter=',')\n",
    "#df = pd.read_csv(\"cleaned_plain-text_labeled_term+combined_no_ref_no_cit_def_same_len_only_must_conatin_B.csv\", delimiter=',') # len = 13692\n",
    "df = pd.read_csv(\"cleaned_plain-text_labeled_term+combined_no_ref_no_cit_def_same_len_only_must_conatin_B_less_than_500tokens.csv\", delimiter=',')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dec0d7b3-6c92-4225-afff-f2c4ca3c7df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "      <th>plain_text_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nLet G be a finite group and let X→ S be a fa...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>admissible G-cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n  Let π∈_n and let x,y be a pair of rooks th...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O B-...</td>\n",
       "      <td>light reduction pair;heavy reduction pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A local modification rule is a pair (A,T), w...</td>\n",
       "      <td>O B-MATH_TERM I-MATH_TERM I-MATH_TERM O O O O ...</td>\n",
       "      <td>Local modification rule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A vertex set is any set which is at most cou...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O B-MATH_TERM O ...</td>\n",
       "      <td>Vertex sets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A finite palette is a sequence K = (K_j)_j=0^∞...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>Palettes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12624</th>\n",
       "      <td>\\n\\tA plabic graph is an undirected graph G dr...</td>\n",
       "      <td>O B-MATH_TERM I-MATH_TERM O O O O O O O O O O ...</td>\n",
       "      <td>plabic graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12625</th>\n",
       "      <td>\\n\\tFor a plabic graph G, the trip π_G describ...</td>\n",
       "      <td>O O O O O O O O O O O O O B-MATH_TERM I-MATH_T...</td>\n",
       "      <td>decorated trip permutation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12626</th>\n",
       "      <td>\\n\\tThe matroid polytope Γ_M of the matroid M ...</td>\n",
       "      <td>O B-MATH_TERM I-MATH_TERM O O O O O O O O O O ...</td>\n",
       "      <td>matroid polytope;positroid polytope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12627</th>\n",
       "      <td>[Maximal rooted rainbow tree]  Given r≥ 3, let...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>such that ; such that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12628</th>\n",
       "      <td>\\nGiven a 2-graph F with v vertices and e edg...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>c-supersaturates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12629 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      \\nLet G be a finite group and let X→ S be a fa...   \n",
       "1      \\n  Let π∈_n and let x,y be a pair of rooks th...   \n",
       "2        A local modification rule is a pair (A,T), w...   \n",
       "3        A vertex set is any set which is at most cou...   \n",
       "4      A finite palette is a sequence K = (K_j)_j=0^∞...   \n",
       "...                                                  ...   \n",
       "12624  \\n\\tA plabic graph is an undirected graph G dr...   \n",
       "12625  \\n\\tFor a plabic graph G, the trip π_G describ...   \n",
       "12626  \\n\\tThe matroid polytope Γ_M of the matroid M ...   \n",
       "12627  [Maximal rooted rainbow tree]  Given r≥ 3, let...   \n",
       "12628   \\nGiven a 2-graph F with v vertices and e edg...   \n",
       "\n",
       "                                             word_labels  \\\n",
       "0      O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "1      O O O O O O O O O O O O O O O O O O O O O O B-...   \n",
       "2      O B-MATH_TERM I-MATH_TERM I-MATH_TERM O O O O ...   \n",
       "3      O O O O O O O O O O O O O O O O B-MATH_TERM O ...   \n",
       "4      O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "...                                                  ...   \n",
       "12624  O B-MATH_TERM I-MATH_TERM O O O O O O O O O O ...   \n",
       "12625  O O O O O O O O O O O O O B-MATH_TERM I-MATH_T...   \n",
       "12626  O B-MATH_TERM I-MATH_TERM O O O O O O O O O O ...   \n",
       "12627  O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "12628  O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "\n",
       "                                 plain_text_term  \n",
       "0                             admissible G-cover  \n",
       "1      light reduction pair;heavy reduction pair  \n",
       "2                        Local modification rule  \n",
       "3                                    Vertex sets  \n",
       "4                                       Palettes  \n",
       "...                                          ...  \n",
       "12624                               plabic graph  \n",
       "12625                 decorated trip permutation  \n",
       "12626        matroid polytope;positroid polytope  \n",
       "12627                     such that ; such that   \n",
       "12628                           c-supersaturates  \n",
       "\n",
       "[12629 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = df[['plain_text_def','labeled_def','plain_text_term']].copy()\n",
    "\n",
    "all_data.rename(columns={\"plain_text_def\": \"sentence\", \"labeled_def\": \"word_labels\" }, inplace=True)\n",
    "\n",
    "all_data['word_labels'] = all_data['word_labels'].str.replace('I_MATH_TERM','I-MATH_TERM')\n",
    "all_data['word_labels'] = all_data['word_labels'].str.replace('B_MATH_TERM','B-MATH_TERM')\n",
    "\n",
    "\n",
    "data = all_data[:-1024] #make a small sample first\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e9a1c11-92c4-4ade-bf5d-a57cff09e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "      <th>plain_text_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12629</th>\n",
       "      <td>Let P be a G-poset. The compatibility graph of...</td>\n",
       "      <td>O O O O O O B-MATH_TERM I-MATH_TERM O O O O O ...</td>\n",
       "      <td>Compatibility graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12630</th>\n",
       "      <td>Let P be a G-poset. The strong compatibility g...</td>\n",
       "      <td>O O O O O O B-MATH_TERM I-MATH_TERM I-MATH_TER...</td>\n",
       "      <td>Strong compatibility graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12631</th>\n",
       "      <td>\\nLet p=p_1p_2⋯ p_n be a permutation. \\nWe say...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O B-MATH...</td>\n",
       "      <td>good pair;bad pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12632</th>\n",
       "      <td>\\nThe family of probability measures on partit...</td>\n",
       "      <td>O O O O O O O O O O B-MATH_TERM O O O O O O O ...</td>\n",
       "      <td>multiplicative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12633</th>\n",
       "      <td>\\nWe call a family of measures μ^(n) ergodic i...</td>\n",
       "      <td>O O O O O O O B-MATH_TERM O O O O O O O O O O ...</td>\n",
       "      <td>ergodic;limit shape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13648</th>\n",
       "      <td>Consider ⟨ G→ S, α→ A⟩, two adjacent vertices...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>preferred direction;special claw corresponding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13649</th>\n",
       "      <td>\\nAccording to Theorem &lt;ref&gt;, ^d has exactly d...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>Cyclicity classes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13650</th>\n",
       "      <td>By Wielandt number we mean the following funct...</td>\n",
       "      <td>O B-MATH_TERM I-MATH_TERM O O O O O O O O O O O O</td>\n",
       "      <td>Wielandt number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13651</th>\n",
       "      <td>\\nThe girth of , denoted it by g(),\\nis the sm...</td>\n",
       "      <td>O B-MATH_TERM O O O O O O O O O O O O O O O O</td>\n",
       "      <td>Girth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13652</th>\n",
       "      <td>\\nLet A∈. The cycles of (A) at which the maxim...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>Critical graph</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "12629  Let P be a G-poset. The compatibility graph of...   \n",
       "12630  Let P be a G-poset. The strong compatibility g...   \n",
       "12631  \\nLet p=p_1p_2⋯ p_n be a permutation. \\nWe say...   \n",
       "12632  \\nThe family of probability measures on partit...   \n",
       "12633  \\nWe call a family of measures μ^(n) ergodic i...   \n",
       "...                                                  ...   \n",
       "13648   Consider ⟨ G→ S, α→ A⟩, two adjacent vertices...   \n",
       "13649  \\nAccording to Theorem <ref>, ^d has exactly d...   \n",
       "13650  By Wielandt number we mean the following funct...   \n",
       "13651  \\nThe girth of , denoted it by g(),\\nis the sm...   \n",
       "13652  \\nLet A∈. The cycles of (A) at which the maxim...   \n",
       "\n",
       "                                             word_labels  \\\n",
       "12629  O O O O O O B-MATH_TERM I-MATH_TERM O O O O O ...   \n",
       "12630  O O O O O O B-MATH_TERM I-MATH_TERM I-MATH_TER...   \n",
       "12631  O O O O O O O O O O O O O O O O O O O O B-MATH...   \n",
       "12632  O O O O O O O O O O B-MATH_TERM O O O O O O O ...   \n",
       "12633  O O O O O O O B-MATH_TERM O O O O O O O O O O ...   \n",
       "...                                                  ...   \n",
       "13648  O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "13649  O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "13650  O B-MATH_TERM I-MATH_TERM O O O O O O O O O O O O   \n",
       "13651      O B-MATH_TERM O O O O O O O O O O O O O O O O   \n",
       "13652  O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "\n",
       "                                         plain_text_term  \n",
       "12629                                Compatibility graph  \n",
       "12630                         Strong compatibility graph  \n",
       "12631                                 good pair;bad pair  \n",
       "12632                                     multiplicative  \n",
       "12633                                ergodic;limit shape  \n",
       "...                                                  ...  \n",
       "13648  preferred direction;special claw corresponding...  \n",
       "13649                                  Cyclicity classes  \n",
       "13650                                    Wielandt number  \n",
       "13651                                              Girth  \n",
       "13652                                     Critical graph  \n",
       "\n",
       "[1024 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_data = all_data[-1024:]\n",
    "gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b6cac1-7450-4879-bedc-4e927c279b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = gen_data[-1024:]\n",
    "gen_data.to_csv('data/test_GPT+labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3761565-81a4-4b6f-95d7-5b67156d53f4",
   "metadata": {},
   "source": [
    "# Preparing the dataset and dataloader\n",
    "\n",
    "Now that our data is preprocessed, we can turn it into PyTorch tensors such that we can provide it to the model. Let's start by defining some key variables that will be used later on in the training/evaluation process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324c5cdb-31c3-4135-b3a4-db3a15bc5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split()):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b750e01-db44-4825-9eb2-6f6cdbbc73b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-MATH_TERM': 0, 'I-MATH_TERM': 1, 'O': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['B-MATH_TERM', 'I-MATH_TERM', 'O']\n",
    "\n",
    "label2id = { label : labels.index(label) for label in labels}\n",
    "\n",
    "id2label = { labels.index(label) : label for label in labels}\n",
    "\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09be9b4a-0479-42b7-8048-268ba11ef9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]  \n",
    "        word_labels = self.data.word_labels[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"<s> \"] + tokenized_sentence + [\" </s>\"] # add special tokens of Roberta\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.append(\"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['<pad>'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '<pad>' else 0 for tok in tokenized_sentence] #modifié selon https://huggingface.co/docs/transformers/v4.21.1/en/model_doc/camembert\n",
    "        \n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c0fa89-f24a-45c6-af71-6375067f5566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"InriaValda/cc_math_roberta_ep01\", from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45ff33-0dba-406d-9923-faefb5257a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splite dataset and load for the first time\n",
    "\"\"\"train_size = 0.9\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "val_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "#save the data sets\n",
    "train_dataset.to_csv('data/train.csv', index=False)\n",
    "val_dataset.to_csv('data/val.csv', index=False)\n",
    "gen_data.to_csv('data/test.csv', index=False)\n",
    "\n",
    "print(\"FULL TrainigDataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_dataset.shape))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c155182-5651-44c6-a323-4ee641ccdb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold splite dataset and load for the first time\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "skf = KFold(n_splits=10)\n",
    "n = 1                                                        \n",
    "for train_index, val_index in skf.split(X=data['sentence'].to_numpy(), y=data['word_labels'].to_numpy()):\n",
    "    train_set = data.iloc[train_index]\n",
    "    val_set = data.iloc[val_index]\n",
    "    train_file_name = 'data/10-fold/train_499_' + str(n) + '.csv'\n",
    "    train_set.to_csv(train_file_name, index = False)\n",
    "    val_file_name = 'data/10-fold/val_499_' + str(n) + '.csv'\n",
    "    val_set.to_csv(val_file_name, index = False)\n",
    "    n += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6767b-1a20-4cb2-a9a2-91dda83f3685",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## verify tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf3f081-0ded-45e7-a591-365301098a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 18:57:54.770841: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"InriaValda/cc_math_roberta_ep01\",\n",
    "                                                                    from_tf=True,\n",
    "                                                                    num_labels=len(id2label),\n",
    "                                                                    id2label=id2label,\n",
    "                                                                    label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06fc5b73-6356-4e8f-aa98-259240ddbeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([    3,    37,  4172,  1530,  2486, 17040,   309,    69,  8060,    12,\n",
       "            37,    16,    56,   414,  2723,    37,   309,    69,  4270,  3539,\n",
       "            16,   562,    56,    30,    22,    66,    37, 44419,    63,    22,\n",
       "            65,    22, 44911,    20,    16,    21,    97,   309,    69,  8645,\n",
       "          7599,  8838,   266,    37,  4163,    63,    22,    65,  1694,    95,\n",
       "            20,    16,    21,  1472,  2723,    63,    22,    65, 13548,    95,\n",
       "            21,    16,    22,  1472,  3014,   309,  4605,  2758,   761,   508,\n",
       "          1694,  4372,  4165,   511,  1574,    21,   562,    22, 28521,    18,\n",
       "         14602, 12663,  6404,  3539,    58,    16,  2776, 35767,    69,  1530,\n",
       "          2486,  8645,    56,    66,    12,    58,  2213,    22,    66,    37,\n",
       "         44419,    58,    22,  1004,    22,    66,    58,    22,  2879,  4555,\n",
       "          5893,   314,   306,  4888,    12,    90,    67,    21,    16,    90,\n",
       "            67,    22,    13,   274,    58,  1694, 14743,   274,    56,    66,\n",
       "            12,    58,  1202,    43,    13,  1636, 10037,    43,   676,    22,\n",
       "            66,    37, 44419,    58,    22,   439,   562,  9145,   439,    56,\n",
       "            12,    95,    20,    16,    21,    97,    67,    22,    66,    12,\n",
       "           801,   248,   227,    67,    37,  7609,  1135,  1202,    43,    13,\n",
       "            13,    33,    21,    16,  2723,   801,   248,   227,    67,    37,\n",
       "          7609,  1135,    30,    37, 44419,    63,    22,    65,  1004,    37,\n",
       "         44419,    95,    90,    67,    21,    16,    90,    67,    22,    97,\n",
       "           309,  1574,  8645,  3014,   309,  1574, 18049,   266,    37,   562,\n",
       "         14485,    21,    16,    22,  1694,    90,    67,    21,    16,    90,\n",
       "            67,    22,  5969,    18,     3,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'targets': tensor([2, 2, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = dataset(pd.read_csv('data/val.csv'), tokenizer, MAX_LEN)\n",
    "\n",
    "validation_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3eccec3-630b-4e09-9fe3-3578a6fbef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>            O\n",
      "For              O\n",
      "k                O\n",
      "âī¥              O\n",
      "2                O\n",
      ",                O\n",
      "a                O\n",
      "k                B-MATH_TERM\n",
      "-                B-MATH_TERM\n",
      "tensor           B-MATH_TERM\n",
      "with             O\n",
      "entries          O\n",
      "in               O\n",
      "is               O\n",
      "a                O\n",
      "function         O\n",
      "T                O\n",
      ":{               O\n",
      "1                O\n",
      ",...,            O\n",
      "d                O\n",
      "}                O\n",
      "^                O\n",
      "k                O\n",
      "âŁ               O\n",
      "¶                O\n",
      ".                O\n",
      "We               O\n",
      "refer            O\n",
      "to               O\n",
      "the              O\n",
      "number           O\n",
      "k                O\n",
      "as               O\n",
      "the              O\n",
      "order            O\n",
      "of               O\n",
      "the              O\n",
      "tensor           O\n",
      "T                O\n",
      ".                O\n",
      "We               O\n",
      "denote           O\n",
      "by               O\n",
      "T                O\n",
      "_                O\n",
      "i                O\n",
      "_                O\n",
      "1                O\n",
      "âĭ¯              O\n"
     ]
    }
   ],
   "source": [
    "# print the first 50 tokens and corresponding labels\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(validation_set[15][\"ids\"][:50]), validation_set[15][\"targets\"][:50]):\n",
    "  print('{0:15}  {1}'.format(token, id2label[label.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1138d7c0-2259-4bcb-b702-5a764d25dcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>            O\n",
      "The              O\n",
      "mixed            B-MATH_TERM\n",
      "volume           I-MATH_TERM\n",
      "(                O\n",
      "P                O\n",
      "_                O\n",
      "1                O\n",
      ",                O\n",
      "âĢ               O\n",
      "¦                O\n",
      ",                O\n",
      "P                O\n",
      "_                O\n",
      "n                O\n",
      ")                O\n",
      "is               O\n",
      "the              O\n",
      "coefficient      O\n",
      "of               O\n",
      "the              O\n",
      "monomial         O\n",
      "_                O\n",
      "1                O\n",
      "âĭ¯              O\n",
      "_                O\n",
      "n                O\n",
      "in               O\n",
      "the              O\n",
      "polynomial       O\n"
     ]
    }
   ],
   "source": [
    "# print the first 50 tokens and corresponding labels\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(validation_set[49][\"ids\"][:30]), validation_set[49][\"targets\"][:30]):\n",
    "  print('{0:15}  {1}'.format(token, id2label[label.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "733d98d9-a0de-46fc-a69b-2ea3bb03ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intial loss = 1.147135853767395\n"
     ]
    }
   ],
   "source": [
    "# 3 labels: -ln(1/3) = 1.09861228867\n",
    "ids = validation_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = validation_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = validation_set[0][\"targets\"].unsqueeze(0)\n",
    "\n",
    "ids = ids.to(device)#, dtype = torch.long)\n",
    "mask = mask.to(device)#, dtype = torch.long)\n",
    "targets = targets.to(device)#, dtype = torch.long)\n",
    "model.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "\n",
    "print(f\"intial loss = {initial_loss.item()}\")\n",
    "# it seems that initial loss grows with the pretraining epochs of cc-xxxbert "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78edbdb5-869f-4f4e-ad04-216bd6e83cba",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdc82b2-cc26-43b4-9c6d-b5ccbad3e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(model, training_loader, optimizer, scheduler=None):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        '''\n",
    "        loss, tr_logits  = model(input_ids=ids, attention_mask=mask, labels=targets)#temporary modification for transformer 3'''\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        #if idx % 100==0:\n",
    "        #    loss_step = tr_loss/nb_tr_steps\n",
    "        #    print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    #print(f\"Trained {nb_tr_steps} steps\")\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    \n",
    "\n",
    "def valid(model, validation_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(validation_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "           \n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            #if idx % 100==0:\n",
    "            #    loss_step = eval_loss/nb_eval_steps\n",
    "            #    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions\n",
    "\n",
    "def print_reports_to_csv(test_results, model_name, LEARNING_RATE, EPOCHS, trainset_num, report_type):\n",
    "    test_reports = []\n",
    "    for res in test_results:\n",
    "        report = classification_report([res['labels']], [res['predictions']], output_dict=True)\n",
    "        flattened_report = {str(k+'_'+v_k) : v_v for k,v in report.items() for v_k, v_v in v.items()  }\n",
    "        flattened_report['trainset_size'] = res['trainset_size']\n",
    "        flattened_report['model'] = res['model']\n",
    "        flattened_report['trainset_num'] = trainset_num\n",
    "        test_reports.append(flattened_report)\n",
    "    \n",
    "    df_test_reports = pd.DataFrame(test_reports)\n",
    "    if '/' in model_name:\n",
    "        model_name =  model_name.split('/')[1] \n",
    "    test_report_name = f'finetuning_results/{report_type}_{model_name}_{LEARNING_RATE}_16_{EPOCHS}.csv'\n",
    "    df_test_reports.to_csv(test_report_name, mode='a', header=not os.path.exists(test_report_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede47df-7d18-4d7b-bc14-a2323ddf63dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (2048, 3)\n",
      "tranining steps: 1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss epoch: 0.08286487784062047\n",
      "Training accuracy epoch: 0.952549218477001\n",
      "Validation Loss: 0.045093063005729565\n",
      "Validation Accuracy: 0.9660932983587777\n",
      "Training epoch: 2\n",
      "Training loss epoch: 0.039080586153431796\n",
      "Training accuracy epoch: 0.9687126276914014\n",
      "Validation Loss: 0.03779354774027686\n",
      "Validation Accuracy: 0.9714530980732353\n",
      "Training epoch: 3\n",
      "Training loss epoch: 0.029751134941761848\n",
      "Training accuracy epoch: 0.9761024446076665\n",
      "Validation Loss: 0.03717532100839705\n",
      "Validation Accuracy: 0.9739979009855233\n",
      "Training epoch: 4\n",
      "Training loss epoch: 0.02324110506742727\n",
      "Training accuracy epoch: 0.9809312977370916\n",
      "Validation Loss: 0.04294008636682094\n",
      "Validation Accuracy: 0.9707549650917959\n",
      "Training epoch: 5\n",
      "Training loss epoch: 0.017480263977631694\n",
      "Training accuracy epoch: 0.9857224869288685\n",
      "Validation Loss: 0.03787283281076558\n",
      "Validation Accuracy: 0.9701824570633991\n",
      "Training epoch: 6\n",
      "Training loss epoch: 0.01421614603168564\n",
      "Training accuracy epoch: 0.9887108059100436\n",
      "Validation Loss: 0.047532850758561604\n",
      "Validation Accuracy: 0.9749227784317647\n",
      "Training epoch: 7\n",
      "Training loss epoch: 0.012013390580250416\n",
      "Training accuracy epoch: 0.9905343480171308\n",
      "Validation Loss: 0.04880466137694407\n",
      "Validation Accuracy: 0.9745102920723201\n",
      "Training epoch: 8\n",
      "Training loss epoch: 0.009808328417420853\n",
      "Training accuracy epoch: 0.992330816508168\n",
      "Validation Loss: 0.050601358525455\n",
      "Validation Accuracy: 0.9745215510059284\n",
      "Training epoch: 9\n",
      "Training loss epoch: 0.00853482938509842\n",
      "Training accuracy epoch: 0.9930689261499976\n",
      "Validation Loss: 0.05131061547258987\n",
      "Validation Accuracy: 0.9740880233174032\n",
      "Training epoch: 10\n",
      "Training loss epoch: 0.006961515326111112\n",
      "Training accuracy epoch: 0.9944954960172258\n",
      "Validation Loss: 0.056165834346526784\n",
      "Validation Accuracy: 0.9747158053636698\n",
      "Validation Loss: 0.056175277910277814\n",
      "Validation Accuracy: 0.9748815158544328\n",
      "Validation Loss: 0.049307242123177275\n",
      "Validation Accuracy: 0.9766643481764747\n",
      "TRAIN Dataset: (2048, 3)\n",
      "tranining steps: 1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss epoch: 0.08320464556163643\n",
      "Training accuracy epoch: 0.9511886245709282\n",
      "Validation Loss: 0.06663252182210548\n",
      "Validation Accuracy: 0.942584903432888\n",
      "Training epoch: 2\n",
      "Training loss epoch: 0.03984326004865579\n",
      "Training accuracy epoch: 0.9683081518191731\n",
      "Validation Loss: 0.040605586965250066\n",
      "Validation Accuracy: 0.9680821112118954\n",
      "Training epoch: 3\n",
      "Training loss epoch: 0.029163400067773182\n",
      "Training accuracy epoch: 0.9766139774371558\n",
      "Validation Loss: 0.03804644028644396\n",
      "Validation Accuracy: 0.971415509613532\n",
      "Training epoch: 4\n",
      "Training loss epoch: 0.02224438277335139\n",
      "Training accuracy epoch: 0.9817856261621576\n",
      "Validation Loss: 0.039472024038999895\n",
      "Validation Accuracy: 0.9718737103973145\n",
      "Training epoch: 5\n",
      "Training loss epoch: 0.01834763454462518\n",
      "Training accuracy epoch: 0.9849037550854435\n",
      "Validation Loss: 0.0450995744123489\n",
      "Validation Accuracy: 0.9730561669943274\n",
      "Training epoch: 6\n",
      "Training loss epoch: 0.014575396045984235\n",
      "Training accuracy epoch: 0.9883641054456178\n",
      "Validation Loss: 0.041475612988483304\n",
      "Validation Accuracy: 0.9714833655865868\n",
      "Training epoch: 7\n",
      "Training loss epoch: 0.01255133413724252\n",
      "Training accuracy epoch: 0.9894920729971248\n",
      "Validation Loss: 0.04339311303146466\n",
      "Validation Accuracy: 0.9740047266708642\n",
      "Training epoch: 8\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#training\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 0\n",
    "            }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "             }\n",
    "\n",
    "\n",
    "for trainset_num in range(3, 5):\n",
    "\n",
    "    train_file_name = f'data/10-fold/train_499_{trainset_num}.csv'#'data/train.csv'\n",
    "    val_file_name = f'data/10-fold/val_499_{trainset_num}.csv'#'data/val.csv'\n",
    "    \n",
    "    for model_name in ['InriaValda/cc_math_roberta_ep01']: #'InriaValda/cc_math_roberta_ep10',\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, from_tf=True, model_max_length=MAX_LEN)\n",
    "        \n",
    "        test_generalizability_set = dataset(pd.read_csv('data/test_GPT+labels.csv'), tokenizer, MAX_LEN)\n",
    "        \n",
    "        validation_set = dataset(pd.read_csv(val_file_name), tokenizer, MAX_LEN)\n",
    "        df_training_set = pd.read_csv(train_file_name)\n",
    "        \n",
    "        val_results = []\n",
    "        test_results = []\n",
    "        \n",
    "        validation_loader = DataLoader(validation_set, **val_params)\n",
    "        test_gen_loader = DataLoader(test_generalizability_set, **val_params)\n",
    "        \n",
    "        for trainsetsize in [2048]:  #[64,128,256,512,1024,2048,4096,8192,11401] are already done\n",
    "            training_set = dataset(df_training_set[:trainsetsize], tokenizer, MAX_LEN)\n",
    "        \n",
    "            print(\"TRAIN Dataset: {}\".format(training_set.data.shape))\n",
    "            #train_params['batch_size'] =  int( trainsetsize / 32) if (trainsetsize < 1024) else 16\n",
    "            training_loader = DataLoader(training_set, **train_params)\n",
    "        \n",
    "        \n",
    "            num_training_steps = int(training_loader.dataset.len / train_params['batch_size'] * EPOCHS)\n",
    "            print(f'tranining steps: {num_training_steps+1}')\n",
    "        \n",
    "            #Shrey uses TF model\n",
    "            model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                                    from_tf=True,\n",
    "                                                                    num_labels=len(id2label),\n",
    "                                                                    id2label=id2label,\n",
    "                                                                    label2id=label2id)\n",
    "            model.to(device)\n",
    "        \n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "            #scheduler = get_cosine_schedule_with_warmup(optimizer = optimizer, num_warmup_steps = 50, num_training_steps=num_training_steps)\n",
    "            for epoch in range(EPOCHS):\n",
    "            #for epoch in range(flex_epoch_nb): \n",
    "                print(f\"Training epoch: {epoch + 1}\")\n",
    "                train(model, training_loader, optimizer)\n",
    "                valid(model, validation_loader)\n",
    "                #valid(model, test_gen_loader)\n",
    "            labels, predictions = valid(model, validation_loader)     \n",
    "            val_results.append({'trainset_size': trainsetsize, 'model': model_name, 'labels': labels, 'predictions': predictions})\n",
    "        \n",
    "            #test generalizablity\n",
    "            labels, predictions = valid(model, test_gen_loader)\n",
    "            test_results.append({'trainset_size': trainsetsize, 'model': model_name, 'labels': labels, 'predictions': predictions})\n",
    "            ner_model_name = f'ner_model/{model_name}_ft_{EPOCHS}ep_train_size_{trainsetsize}_trainset_{trainset_num}'\n",
    "            model.save_pretrained(ner_model_name)\n",
    "            tokenizer.save_pretrained(ner_model_name)\n",
    "            # gpt_aligned_eval(model, tokenizer, ner_model_name) # too slow!\n",
    "        \n",
    "        print_reports_to_csv(val_results, model_name, LEARNING_RATE, EPOCHS, trainset_num, 'validation')\n",
    "        print_reports_to_csv(test_results, model_name, LEARNING_RATE, EPOCHS, trainset_num, 'generalizability')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c5387-7022-43e9-9fe3-d838e7454b87",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The fun part is when we can quickly test the model on new, unseen sentences. Here, we use the prediction of the first word piece of every word. Note that the function we used to train our model (tokenze_and_preserve_labels) propagated the label to all subsequent word pieces (so you could for example also perform a majority vote on the predicted labels of all word pieces of a word).\n",
    "\n",
    "In other words, the code below does not take into account when predictions of different word pieces that belong to the same word do not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62aa50e1-2f81-4be4-bd58-cdc7ec431a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MATH_TERM',\n",
       "  'score': 0.8791002,\n",
       "  'word': ' Betti poset',\n",
       "  'start': 4,\n",
       "  'end': 15}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = AutoModelForTokenClassification.from_pretrained('ner_model/')\n",
    "pipe = pipeline(task=\"token-classification\", model=model.to('cpu'), tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "pipe(\"The Betti poset of a poset P is the subposet consisting of all homologically contributing elements, B(P)={q∈ P  | _i(Δ_q) ≠ 0  i}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1097e81f-551d-4a5c-b6bd-72c92b3021aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.33 s, sys: 30 ms, total: 3.36 s\n",
      "Wall time: 471 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MATH_TERM',\n",
       "  'score': 0.8962922,\n",
       "  'word': ' trivial normal holonomy',\n",
       "  'start': 40,\n",
       "  'end': 63}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe(\"A subskeleton (Γ_0,α_0,θ_0)⊆(Γ,α,θ) has trivial normal holonomy if the holonomy map K_γ^⊥ is trivial for all loops γ⊂Γ_0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d4e16a-5a41-4b18-89ea-88cccfdf0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MATH_TERM',\n",
       "  'score': 0.8791002,\n",
       "  'word': ' Betti poset',\n",
       "  'start': 4,\n",
       "  'end': 15}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ner_model/InriaValda/cc_math_roberta_ep10_ft_3ep_train_size_11366/'\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = pipeline(task=\"token-classification\", model=model.to('cpu'), tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "%time\n",
    "pipe(\"The Betti poset of a poset P is the subposet consisting of all homologically contributing elements, B(P)={q∈ P  | _i(Δ_q) ≠ 0  i}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fdda9-e210-41a9-a931-f45638dc055e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
